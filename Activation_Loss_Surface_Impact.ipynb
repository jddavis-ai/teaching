{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Activation Function and Loss Surface in Gradient Descent\n", "\n", "Activation functions play a critical role in shaping the loss surface used during training. Their non-linear transformations can significantly impact how gradient descent converges to an optimal solution."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Conceptual Comparison\n", "\n", "The diagram below compares two loss surfaces:\n", "- **Without Activation (Linear):** The loss surface is smooth and shallow, which can lead to slow convergence due to small gradients.\n", "- **With Activation (Non-linear):** The loss surface is more curved, offering sharper gradients and potentially faster convergence.\n", "\n", "This illustrates how activation functions enhance learning efficiency by modifying the geometry of the optimization landscape."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from IPython.display import Image\n", "Image(filename='Activation_Loss_Surface_Impact.png')"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": ""}}, "nbformat": 4, "nbformat_minor": 5}