{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09b5d509"
      },
      "source": [
        "# Neural Networks: Forward and Backward Propogation\n",
        "\n",
        "This short tutorial focuses on forward and backward propagation, key processes in training neural networks. We will also compare how these processes occur in different network architectures, specifically feedforward and recurrent neural networks, and demonstrate the forward and backward passes with a simple feedforward network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e842908"
      },
      "source": [
        "## Core Concepts: Forward and Backward Propagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f4733a7"
      },
      "source": [
        "### Forward Propagation\n",
        "\n",
        "Forward propagation is the process of calculating the output of a neural network given its inputs and current weights and biases. It involves passing the input data through each layer of the network, from the input layer to the output layer.\n",
        "\n",
        "Here's a simplified breakdown:\n",
        "\n",
        "1.  **Input Layer:** The input data is fed into the input layer.\n",
        "2.  **Hidden Layers (if any):** For each neuron in a hidden layer, a weighted sum of the inputs from the previous layer is calculated, and an activation function is applied to this sum. The output of this activation function becomes the input for the next layer.\n",
        "3.  **Output Layer:** The same process of weighted sum and activation is applied to the final layer to produce the network's output.\n",
        "\n",
        "The purpose of forward propagation is to generate a prediction or output from the neural network based on the current state of its parameters. This output is then compared to the actual target value to calculate the error.  Target output is calculated from the ground truth data or actual labels for each data point.  When training, conventionally, we provide data that includes data points and a label.  See the tutorial on preparing data for an neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a733be7"
      },
      "source": [
        "### Backward Propagation\n",
        "\n",
        "Backpropagation (short for \"backward propagation of errors\") is the key algorithm used to train neural networks. Its primary purpose is to calculate the gradients of the network's loss function with respect to each weight and bias. These gradients indicate how much the loss function will change with a small change in each parameter.\n",
        "\n",
        "Here's a simplified breakdown of what happens during training:\n",
        "\n",
        "1.  **Error Calculation:** After forward propagation, the output of the network is compared to the expected output (the ground truth), and an error (or loss) is calculated.\n",
        "2.  **Gradient Calculation (Backward Pass):** Backpropagation starts from the output layer and moves backward through the network towards the input layer. At each layer, it calculates the contribution of each weight and bias to the total error using the chain rule of calculus. This process determines how much each parameter needs to be adjusted to reduce the error.\n",
        "3.  **Parameter Update:** Once the gradients are calculated, an optimization algorithm (like gradient descent) uses these gradients to update the weights and biases. The parameters are adjusted in a direction that minimizes the loss function.\n",
        "\n",
        "Backpropagation essentially tells the network how to change its internal parameters to make better predictions in the future."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52a759bd"
      },
      "source": [
        "### Relationship Between Forward and Backward Propagation\n",
        "\n",
        "Forward propagation and backpropagation are two essential, sequential processes in the training of a neural network:\n",
        "\n",
        "1.  **Forward Propagation:** This is the **initial step**. It takes the input data and pushes it through the network to generate an output (prediction). This process uses the current weights and biases to compute the output and then calculates the error between the predicted output and the actual target value.\n",
        "2.  **Backward Propagation:** This is the **subsequent step**. It uses the error calculated during forward propagation to determine how much each weight and bias in the network contributed to that error. By calculating gradients, backpropagation provides the necessary information for the optimization algorithm to adjust the network's parameters.\n",
        "\n",
        "In essence, forward propagation provides the outcome and the measure of its inaccuracy (the error), while backpropagation provides the necessary information (the gradients) to *correct* that inaccuracy by updating the network's parameters. Forward propagation feeds information forward to get an output, and backpropagation flows backward to adjust the parameters based on the output's error. This cycle of forward and backward passes is repeated many times during training until the network's parameters are optimized and the error is minimized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddd0b01d"
      },
      "source": [
        "## Forward and Backward Passes in Different Architectures\n",
        "\n",
        "The core principles of forward and backward propagation apply to various neural network architectures, but their implementation and the flow of information can differ.\n",
        "\n",
        "### Feedforward Neural Networks (FNNs)\n",
        "\n",
        "In a feedforward network, the connections between neurons only flow in one directionâ€”from the input layer through the hidden layers (if any) to the output layer. There are no cycles or loops.\n",
        "\n",
        "*   **Forward Pass:** The input data travels directly through the network, layer by layer, with each neuron's output calculated based on the weighted sum of its inputs from the *previous* layer. This is a straightforward, one-way computation.\n",
        "*   **Backward Pass:** The error is calculated at the output layer and then propagated backward through the network. Gradients are calculated for each layer based on the gradients of the * subsequent* layer and the activation values from the forward pass. The chain rule is applied layer by layer from the output back to the input.\n",
        "\n",
        "### Recurrent Neural Networks (RNNs)\n",
        "\n",
        "Recurrent neural networks are designed to handle sequential data. They have connections that loop back on themselves, allowing information to persist and influence the processing of subsequent inputs. This introduces a time dimension to the network's behavior.\n",
        "\n",
        "*   **Forward Pass:** The forward pass in an RNN involves processing the input sequence one step at a time. At each time step, the network receives the current input and the hidden state from the *previous* time step. The hidden state is updated based on the current input and the previous hidden state, and an output is produced. This means the forward pass is not just a single pass but a sequence of passes over time.\n",
        "*   **Backward Pass:** Training RNNs involves a technique called **Backpropagation Through Time (BPTT)**. BPTT essentially unfolds the recurrent network over the entire sequence length, treating it as a deep feedforward network where each time step is a layer. The error is calculated at the end of the sequence and then backpropagated through each time step, going backward in time. This process can be computationally expensive and can lead to the vanishing or exploding gradient problem for long sequences.\n",
        "\n",
        "### Key Differences in Forward/Backward Pass due to Architecture\n",
        "\n",
        "| Feature          | Feedforward Neural Network                      | Recurrent Neural Network (RNN)                    |\n",
        "| :--------------- | :---------------------------------------------- | :------------------------------------------------ |\n",
        "| **Forward Pass** | Straightforward, one-way computation layer by layer | Sequential processing over time, uses previous hidden state |\n",
        "| **Backward Pass**| Backpropagation layer by layer                  | Backpropagation Through Time (BPTT), unfolds network over time |\n",
        "| **Information Flow** | Unidirectional                                  | Includes loops and cycles, allows information persistence |\n",
        "| **Gradient Calculation** | Standard backpropagation                        | BPTT, can face vanishing/exploding gradient issues |\n",
        "| **Suitable for** | Independent data points                         | Sequential data (time series, text)               |\n",
        "\n",
        "Understanding these differences is crucial for choosing the appropriate network architecture for a given task and for implementing the training process correctly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d516b29"
      },
      "source": [
        "## Demonstrating Forward and Backward Propagation (Feedforward Network)\n",
        "\n",
        "Here, we will use a simple feedforward neural network to illustrate the forward and backward passes computationally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3a532ca"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"Sigmoid activation function.\"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
        "    return x * (1 - x)\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        \"\"\"Initializes the neural network with random weights and biases.\"\"\"\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.weights_input_hidden = np.random.rand(self.input_size, self.hidden_size) * 0.01\n",
        "        self.bias_hidden = np.zeros((1, self.hidden_size))\n",
        "        self.weights_hidden_output = np.random.rand(self.hidden_size, self.output_size) * 0.01\n",
        "        self.bias_output = np.zeros((1, self.output_size))\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"Performs the forward pass through the network.\"\"\"\n",
        "        # Input to hidden layer\n",
        "        self.hidden_layer_input = np.dot(X, self.weights_input_hidden) + self.bias_hidden\n",
        "        self.hidden_layer_output = sigmoid(self.hidden_layer_input)\n",
        "\n",
        "        # Hidden to output layer\n",
        "        self.output_layer_input = np.dot(self.hidden_layer_output, self.weights_hidden_output) + self.bias_output\n",
        "        self.predicted_output = sigmoid(self.output_layer_input)\n",
        "\n",
        "        return self.predicted_output\n",
        "\n",
        "    def backward(self, X, y, predicted_output):\n",
        "        \"\"\"Performs the backward pass through the network.\"\"\"\n",
        "        # Calculate the error at the output layer\n",
        "        output_error = 2 * (predicted_output - y) / y.shape[0]\n",
        "\n",
        "        # Calculate the delta (error signal) at the output layer\n",
        "        output_delta = output_error * sigmoid_derivative(predicted_output)\n",
        "\n",
        "        # Calculate the gradients for the weights and biases between the hidden and output layers\n",
        "        gradients_weights_hidden_output = np.dot(self.hidden_layer_output.T, output_delta)\n",
        "        gradients_bias_output = np.sum(output_delta, axis=0, keepdims=True)\n",
        "\n",
        "        # Calculate the error signal at the hidden layer\n",
        "        hidden_error = np.dot(output_delta, self.weights_hidden_output.T)\n",
        "\n",
        "        # Calculate the delta (error signal) at the hidden layer\n",
        "        hidden_delta = hidden_error * sigmoid_derivative(self.hidden_layer_output)\n",
        "\n",
        "        # Calculate the gradients for the weights and biases between the input and hidden layers\n",
        "        gradients_weights_input_hidden = np.dot(X.T, hidden_delta)\n",
        "        gradients_bias_hidden = np.sum(hidden_delta, axis=0, keepdims=True)\n",
        "\n",
        "        # Return the calculated gradients\n",
        "        return {\n",
        "            'weights_input_hidden': gradients_weights_input_hidden,\n",
        "            'bias_hidden': gradients_bias_hidden,\n",
        "            'weights_hidden_output': gradients_weights_hidden_output,\n",
        "            'bias_output': gradients_bias_output\n",
        "        }"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c22fffd"
      },
      "source": [
        "### Implementing Forward Propagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "31d9dee7",
        "outputId": "edecd3ae-9dd3-4919-c83d-a330de6cae6c"
      },
      "source": [
        "# 1. Define input data X and target output y\n",
        "X = np.array([[0, 0],\n",
        "              [0, 1],\n",
        "              [1, 0],\n",
        "              [1, 1]])\n",
        "\n",
        "y = np.array([[0],\n",
        "              [1],\n",
        "              [1],\n",
        "              [0]])\n",
        "\n",
        "# 2. Instantiate the NeuralNetwork class\n",
        "# Input size is the number of features in X\n",
        "input_size = X.shape[1]\n",
        "# Output size is the number of features in y\n",
        "output_size = y.shape[1]\n",
        "# Choose a hidden layer size\n",
        "hidden_size = 4\n",
        "\n",
        "nn = NeuralNetwork(input_size, hidden_size, output_size)\n",
        "\n",
        "# 3. Call the forward() method\n",
        "predicted_output = nn.forward(X)\n",
        "\n",
        "# 4. Print the input data and the predicted output\n",
        "print(\"Input data (X):\")\n",
        "print(X)\n",
        "print(\"\\nPredicted output after forward pass:\")\n",
        "print(predicted_output)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input data (X):\n",
            "[[0 0]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [1 1]]\n",
            "\n",
            "Predicted output after forward pass:\n",
            "[[0.50173088]\n",
            " [0.50173437]\n",
            " [0.50173677]\n",
            " [0.50174026]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b43debba"
      },
      "source": [
        "### Calculating the Error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "faf5d027",
        "outputId": "9a1177ad-b7a2-4308-a761-a6a3a67d5aa0"
      },
      "source": [
        "# 1. Define the Mean Squared Error (MSE) loss function\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "    \"\"\"Calculates the Mean Squared Error between true and predicted values.\"\"\"\n",
        "    return np.mean((y_true - y_pred)**2)\n",
        "\n",
        "# 2. Calculate the error between the predicted_output and the true target values y\n",
        "error = mean_squared_error(y, predicted_output)\n",
        "\n",
        "# 3. Print the calculated error\n",
        "print(\"\\nTrue target values (y):\")\n",
        "print(y)\n",
        "print(\"\\nCalculated error (MSE):\")\n",
        "print(error)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "True target values (y):\n",
            "[[0]\n",
            " [1]\n",
            " [1]\n",
            " [0]]\n",
            "\n",
            "Calculated error (MSE):\n",
            "0.2500030121868324\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75f6de46"
      },
      "source": [
        "### Implementing Backward Propagation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "059e5dc7",
        "outputId": "08b11f95-f841-425c-d6a0-7f6abd585a73"
      },
      "source": [
        "# Call the backward() method to calculate gradients\n",
        "gradients = nn.backward(X, y, predicted_output)\n",
        "\n",
        "# Print the calculated gradients\n",
        "print(\"\\nGradients (Output of Backward Pass):\")\n",
        "print(\"Gradients weights hidden to output:\")\n",
        "print(gradients['weights_hidden_output'])\n",
        "print(\"\\nGradients bias output:\")\n",
        "print(gradients['bias_output'])\n",
        "print(\"\\nGradients weights input to hidden:\")\n",
        "print(gradients['weights_input_hidden'])\n",
        "print(\"\\nGradients bias hidden:\")\n",
        "print(gradients['bias_hidden'])"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Gradients (Output of Backward Pass):\n",
            "Gradients weights hidden to output:\n",
            "[[0.00043509]\n",
            " [0.00043578]\n",
            " [0.00043503]\n",
            " [0.000434  ]]\n",
            "\n",
            "Gradients bias output:\n",
            "[[0.00086777]]\n",
            "\n",
            "Gradients weights input to hidden:\n",
            "[[6.06738546e-07 5.10538267e-07 1.89077514e-08 3.63114503e-07]\n",
            " [6.04552463e-07 5.10296569e-07 1.89399583e-08 3.62859972e-07]]\n",
            "\n",
            "Gradients bias hidden:\n",
            "[[1.21235786e-06 1.02512665e-06 3.78728578e-08 7.25015605e-07]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90da6554"
      },
      "source": [
        "## Showing the Difference: Forward vs. Backward Pass Outputs\n",
        "\n",
        "Let's compare the outputs of the forward and backward passes to highlight their distinct roles.\n",
        "\n",
        "### Forward Pass Output\n",
        "\n",
        "The output of the forward pass is the network's prediction for the given input data. In our example, the predicted output for the input `X` was:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "d05a63fb",
        "outputId": "1211d37c-59c2-40f6-a526-3516e7788277"
      },
      "source": [
        "print(predicted_output)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.50173088]\n",
            " [0.50173437]\n",
            " [0.50173677]\n",
            " [0.50174026]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c24119d9"
      },
      "source": [
        "This output is a set of values, one for each input sample, representing the network's attempt to match the target output.\n",
        "\n",
        "### Backward Pass Output (Gradients)\n",
        "\n",
        "The output of the backward pass is a set of gradients. These gradients quantify how much the loss function changes with respect to each weight and bias in the network.\n",
        "\n",
        "In our example, the calculated gradients were:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "97917de2",
        "outputId": "dabcecc2-21c3-45d2-96d9-c4f463587c21"
      },
      "source": [
        "print(\"Gradients weights hidden to output:\")\n",
        "print(gradients['weights_hidden_output'])\n",
        "print(\"\\nGradients bias output:\")\n",
        "print(gradients['bias_output'])\n",
        "print(\"\\nGradients weights input to hidden:\")\n",
        "print(gradients['weights_input_hidden'])\n",
        "print(\"\\nGradients bias hidden:\")\n",
        "print(gradients['bias_hidden'])"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradients weights hidden to output:\n",
            "[[0.00043509]\n",
            " [0.00043578]\n",
            " [0.00043503]\n",
            " [0.000434  ]]\n",
            "\n",
            "Gradients bias output:\n",
            "[[0.00086777]]\n",
            "\n",
            "Gradients weights input to hidden:\n",
            "[[6.06738546e-07 5.10538267e-07 1.89077514e-08 3.63114503e-07]\n",
            " [6.04552463e-07 5.10296569e-07 1.89399583e-08 3.62859972e-07]]\n",
            "\n",
            "Gradients bias hidden:\n",
            "[[1.21235786e-06 1.02512665e-06 3.78728578e-08 7.25015605e-07]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a192c0c"
      },
      "source": [
        "These gradients are not predictions but rather indicators of how the network's parameters should be adjusted to reduce the error.\n",
        "\n",
        "### Key Differences in Outputs\n",
        "\n",
        "| Feature          | Forward Pass Output                       | Backward Pass Output (Gradients)                  |\n",
        "| :--------------- | :---------------------------------------- | :------------------------------------------------ |\n",
        "| **Nature**       | Network's prediction                      | Sensitivity of the loss to each parameter         |\n",
        "| **Purpose**      | Generate an output based on current parameters | Inform parameter updates to reduce error          |\n",
        "| **Represents**   | The network's current performance        | How parameters need to change for improvement     |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "605adfd0"
      },
      "source": [
        "## Summary\n",
        "\n",
        "Forward propagation is the process of generating a prediction by passing data through the network, while backward propagation is the process of calculating gradients to understand how to adjust the network's parameters to improve those predictions. Both are essential steps in the training of neural networks, with backpropagation being crucial for enabling the network to learn from its errors. The specific implementation of these passes can vary depending on the neural network architecture, as seen when comparing feedforward networks to recurrent neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "625f2bb3"
      },
      "source": [
        "## Updating Weights and Biases with Gradients\n",
        "\n",
        "After calculating the gradients during the backward pass, the next crucial step in training a neural network is to use these gradients to update the network's weights and biases. This process is guided by an optimization algorithm, the most common of which is Gradient Descent.\n",
        "\n",
        "### Gradient Descent\n",
        "\n",
        "Gradient Descent is an iterative optimization algorithm used to find the minimum of a function. In the context of neural networks, the function we want to minimize is the loss function. The gradients calculated during backpropagation tells us the direction of the steepest increase in the loss function. To minimize the loss, we move in the opposite direction (towards convergence) of the gradient.\n",
        "\n",
        "The update rule for a parameter (weight or bias) using Gradient Descent is:\n",
        "\n",
        "$$ \\text{parameter} = \\text{parameter} - \\text{learning\\_rate} \\times \\text{gradient} $$\n",
        "\n",
        "Where:\n",
        "- $\\text{parameter}$ is the current value of the weight or bias.\n",
        "- $\\text{learning\\_rate}$ is a hyperparameter that controls the step size of the update. A smaller learning rate leads to slower but potentially more stable convergence, while a larger learning rate can speed up convergence but may overshoot the closest minimim weight required for convergence.\n",
        "- $\\text{gradient}$ is the gradient of the loss function with respect to the parameter, calculated during backpropagation.\n",
        "\n",
        "### Implementing Parameter Updates\n",
        "\n",
        "We can now add a method to our `NeuralNetwork` class to update the weights and biases using the calculated gradients and a specified learning rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aae4e82f"
      },
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        \"\"\"Initializes the neural network with random weights and biases.\"\"\"\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.weights_input_hidden = np.random.rand(self.input_size, self.hidden_size) * 0.01\n",
        "        self.bias_hidden = np.zeros((1, self.hidden_size))\n",
        "        self.weights_hidden_output = np.random.rand(self.hidden_size, self.output_size) * 0.01\n",
        "        self.bias_output = np.zeros((1, self.output_size))\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        \"\"\"Sigmoid activation function.\"\"\"\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        \"\"\"Derivative of the sigmoid function.\"\"\"\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"Performs the forward pass through the network.\"\"\"\n",
        "        # Input to hidden layer\n",
        "        self.hidden_layer_input = np.dot(X, self.weights_input_hidden) + self.bias_hidden\n",
        "        self.hidden_layer_output = self.sigmoid(self.hidden_layer_input)\n",
        "\n",
        "        # Hidden to output layer\n",
        "        self.output_layer_input = np.dot(self.hidden_layer_output, self.weights_hidden_output) + self.bias_output\n",
        "        self.predicted_output = self.sigmoid(self.output_layer_input)\n",
        "\n",
        "        return self.predicted_output\n",
        "\n",
        "    def backward(self, X, y, predicted_output):\n",
        "        \"\"\"Performs the backward pass through the network.\"\"\"\n",
        "        # Calculate the error at the output layer\n",
        "        # The error is the difference between the predicted output and the true output (ground truth)\n",
        "        output_error = predicted_output - y\n",
        "        # We then multiply by the derivative of the activation function at the output layer\n",
        "        output_delta = output_error * self.sigmoid_derivative(predicted_output)\n",
        "\n",
        "        # Calculate the gradients for the weights and biases between the hidden and output layers\n",
        "        # Gradients for weights_hidden_output: dot product of hidden layer output transpose and output_delta\n",
        "        gradients_weights_hidden_output = np.dot(self.hidden_layer_output.T, output_delta)\n",
        "        # Gradients for bias_output: sum of output_delta along the rows\n",
        "        gradients_bias_output = np.sum(output_delta, axis=0, keepdims=True)\n",
        "\n",
        "        # Calculate the error signal at the hidden layer\n",
        "        # This error is propagated backward from the output layer\n",
        "        hidden_error = np.dot(output_delta, self.weights_hidden_output.T)\n",
        "\n",
        "        # Calculate the delta (error signal) at the hidden layer\n",
        "        # Multiply the hidden error by the derivative of the activation function at the hidden layer\n",
        "        hidden_delta = hidden_error * self.sigmoid_derivative(self.hidden_layer_output)\n",
        "\n",
        "        # Calculate the gradients for the weights and biases between the input and hidden layers\n",
        "        # Gradients for weights_input_hidden: dot product of input transpose and hidden_delta\n",
        "        gradients_weights_input_hidden = np.dot(X.T, hidden_delta)\n",
        "        # Gradients for bias_hidden: sum of hidden_delta along the rows\n",
        "        gradients_bias_hidden = np.sum(hidden_delta, axis=0, keepdims=True)\n",
        "\n",
        "        # Return the calculated gradients\n",
        "        return {\n",
        "            'weights_input_hidden': gradients_weights_input_hidden,\n",
        "            'bias_hidden': gradients_bias_hidden,\n",
        "            'weights_hidden_output': gradients_weights_hidden_output,\n",
        "            'bias_output': gradients_bias_output\n",
        "        }\n",
        "\n",
        "    def update_parameters(self, gradients, learning_rate):\n",
        "        \"\"\"Updates the weights and biases using the calculated gradients and learning rate.\"\"\"\n",
        "        self.weights_input_hidden -= learning_rate * gradients['weights_input_hidden']\n",
        "        self.bias_hidden -= learning_rate * gradients['bias_hidden']\n",
        "        self.weights_hidden_output -= learning_rate * gradients['weights_hidden_output']\n",
        "        self.bias_output -= learning_rate * gradients['bias_output']"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "516e71ad"
      },
      "source": [
        "### Demonstrating Parameter Update\n",
        "\n",
        "Now, let's demonstrate how to use the calculated gradients to update the network's parameters. We'll use a learning rate of 0.1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "bf6d49e8",
        "outputId": "7c88ebf7-1a4a-45f1-a288-7623c6b58faa"
      },
      "source": [
        "# Re-instantiate the NeuralNetwork class after updating its definition\n",
        "nn = NeuralNetwork(input_size, hidden_size, output_size)\n",
        "\n",
        "# Perform a forward pass to get the predicted output and calculate gradients with the new instance\n",
        "predicted_output = nn.forward(X)\n",
        "gradients = nn.backward(X, y, predicted_output)\n",
        "\n",
        "# Now, call the update_parameters() method\n",
        "learning_rate = 0.1\n",
        "nn.update_parameters(gradients, learning_rate)\n",
        "\n",
        "print(\"\\nWeights and biases after update:\")\n",
        "print(\"Weights input to hidden:\\n\", nn.weights_input_hidden)\n",
        "print(\"\\nBias hidden:\\n\", nn.bias_hidden)\n",
        "print(\"\\nWeights hidden to output:\\n\", nn.weights_hidden_output)\n",
        "print(\"\\nBias output:\\n\", nn.bias_output)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Weights and biases after update:\n",
            "Weights input to hidden:\n",
            " [[0.00425942 0.0019169  0.00885266 0.00940727]\n",
            " [0.00893047 0.0027246  0.00314774 0.00466143]]\n",
            "\n",
            "Bias hidden:\n",
            " [[-1.28307422e-07 -2.29543836e-07 -2.50043725e-07 -1.40688294e-07]]\n",
            "\n",
            "Weights hidden to output:\n",
            " [[0.00368034]\n",
            " [0.00663345]\n",
            " [0.00723572]\n",
            " [0.00404265]]\n",
            "\n",
            "Bias output:\n",
            " [[-0.00013702]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "d9e244f1",
        "outputId": "0d382c10-0843-4d78-f00a-e6c2e2c7384e"
      },
      "source": [
        "# Define a learning rate\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Get the initial weights and biases\n",
        "initial_weights_input_hidden = nn.weights_input_hidden.copy()\n",
        "initial_bias_hidden = nn.bias_hidden.copy()\n",
        "initial_weights_hidden_output = nn.weights_hidden_output.copy()\n",
        "initial_bias_output = nn.bias_output.copy()\n",
        "\n",
        "print(\"Initial weights and biases:\")\n",
        "print(\"Weights input to hidden:\\n\", initial_weights_input_hidden)\n",
        "print(\"\\nBias hidden:\\n\", initial_bias_hidden)\n",
        "print(\"\\nWeights hidden to output:\\n\", initial_weights_hidden_output)\n",
        "print(\"\\nBias output:\\n\", initial_bias_output)\n",
        "\n",
        "# Update the parameters using the calculated gradients and learning rate\n",
        "nn.update_parameters(gradients, learning_rate)\n",
        "\n",
        "print(\"\\nWeights and biases after update:\")\n",
        "print(\"Weights input to hidden:\\n\", nn.weights_input_hidden)\n",
        "print(\"\\nBias hidden:\\n\", nn.bias_hidden)\n",
        "print(\"\\nWeights hidden to output:\\n\", nn.weights_hidden_output)\n",
        "print(\"\\nBias output:\\n\", nn.bias_output)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial weights and biases:\n",
            "Weights input to hidden:\n",
            " [[0.00518564 0.00100597 0.00760695 0.0052026 ]\n",
            " [0.00158254 0.0084511  0.00807895 0.00784457]]\n",
            "\n",
            "Bias hidden:\n",
            " [[-2.04296483e-07 -2.31571760e-07 -1.02931199e-07 -2.82286884e-08]]\n",
            "\n",
            "Weights hidden to output:\n",
            " [[0.00679525]\n",
            " [0.0077105 ]\n",
            " [0.00339879]\n",
            " [0.00088819]]\n",
            "\n",
            "Bias output:\n",
            " [[-0.00011924]]\n",
            "\n",
            "Weights and biases after update:\n",
            "Weights input to hidden:\n",
            " [[0.00518554 0.00100586 0.0076069  0.00520259]\n",
            " [0.00158244 0.00845098 0.0080789  0.00784455]]\n",
            "\n",
            "Bias hidden:\n",
            " [[-4.08592966e-07 -4.63143521e-07 -2.05862398e-07 -5.64573769e-08]]\n",
            "\n",
            "Weights hidden to output:\n",
            " [[0.00673553]\n",
            " [0.00765074]\n",
            " [0.00333894]\n",
            " [0.00082838]]\n",
            "\n",
            "Bias output:\n",
            " [[-0.00023847]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1739cbe3"
      },
      "source": [
        "## Frequently Asked Questions (FAQ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8e0c827"
      },
      "source": [
        "### 1. What is ground truth data and why is it primarily used during training a neural network?\n",
        "\n",
        "**Ground truth data** refers to the actual, correct output or labels for the input data used to train a neural network. It represents the desired outcome that the neural network is trying to predict or approximate.\n",
        "\n",
        "Ground truth data is primarily used **during the training phase** of a neural network because the training process is based on learning from examples. The network makes predictions on the input data, and these predictions are compared against the corresponding ground truth labels. This comparison allows the network to calculate the error or loss. The error signal is then used during backpropagation to adjust the network's internal parameters (weights and biases) in a way that minimizes this error, making the network's future predictions closer to the ground truth.\n",
        "\n",
        "During validation and testing, after the network has been trained, ground truth data is often used to evaluate the network's performance on unseen data, but it is not used to update the network's parameters. The network's goal during inference is to make predictions based on what it has learned from the training data. During inference new output is created (predictions) and compared to the actual corrsponding correct responses.  These can be used to detect accuracy and precision of the neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5efdc18"
      },
      "source": [
        "### 2. What is a loss function, what error does it calculate, and how does it calculate that error?\n",
        "\n",
        "A **loss function** (also known as a cost function or error function) is a mathematical function that quantifies the difference between the output predicted by a neural network and the actual ground truth value. It measures how well the neural network is performing for a given set of parameters and input data.\n",
        "\n",
        "The loss function calculates the **error** between the predicted output and the true output. The specific type of error calculated depends on the task the neural network is designed for. For example:\n",
        "\n",
        "*   **For regression tasks** (predicting continuous values), the error is typically the difference between the predicted continuous value and the true continuous value.\n",
        "*   **For classification tasks** (categorizing data into classes), the error relates to how confidently and correctly the network predicts the class label compared to the true class label.\n",
        "\n",
        "The loss function calculates this error through a specific mathematical formula. Different loss functions have different formulas that are suited for different types of tasks and error measurements. Here are a few common loss functions:\n",
        "\n",
        "*   **Mean Squared Error (MSE):** Commonly used in regression tasks. It calculates the average of the squared differences between the predicted and true values.\n",
        "    $$ MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\n",
        "    where $y_i$ is the true value, $\\hat{y}_i$ is the predicted value, and $n$ is the number of data points.\n",
        "\n",
        "*   **Mean Absolute Error (MAE):** Also used in regression tasks. It calculates the average of the absolute differences between the predicted and true values.\n",
        "    $$ MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| $$\n",
        "\n",
        "*   **Binary Cross-Entropy:** Used in binary classification tasks (two classes). It measures the performance of a classification model whose output is a probability value between 0 and 1.\n",
        "\n",
        "*   **Categorical Cross-Entropy:** Used in multi-class classification tasks (more than two classes). It measures the performance of a classification model where the output is a probability distribution over the classes.\n",
        "\n",
        "The goal during neural network training is to minimize the value of the loss function by adjusting the network's parameters. A lower loss value indicates that the network's predictions are closer to the ground truth."
      ]
    }
  ]
}