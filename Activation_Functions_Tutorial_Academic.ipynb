{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad20c68e"
      },
      "source": [
        "# Activation Functions in Neural Networks â€” A Step-by-Step Tutorial\n",
        "In this tutorial, we'll explore common activation functions used in neural networks.\n",
        "\n",
        "**Introduction to Activation Functions:**\n",
        "Activation functions are crucial components within artificial neural networks, introducing non-linearity into the network's computations. Without activation functions, a neural network, regardless of its depth, would essentially behave like a single-layer linear model, incapable of learning complex patterns or solving non-linear problems.\n",
        "\n",
        "**The XOR Problem and Non-linearity:**\n",
        "A classic example illustrating the need for non-linearity is the XOR (exclusive OR) problem. The XOR function is not linearly separable, meaning you cannot draw a single straight line to separate the inputs that result in an output of 1 from those that result in an output of 0. A neural network without activation functions can only learn linear decision boundaries and thus cannot solve the XOR problem. Activation functions, by introducing non-linearity, allow neural networks to learn complex, non-linear decision boundaries necessary to solve problems like XOR.\n",
        "\n",
        "**Activation Functions Covered:**\n",
        "This notebook will cover the following activation functions:\n",
        "- ReLU (Rectified Linear Unit)\n",
        "- Sigmoid\n",
        "- Tanh (Hyperbolic Tangent)\n",
        "- Leaky ReLU\n",
        "- ELU (Exponential Linear Unit)\n",
        "- Swish\n",
        "- Softplus\n",
        "\n",
        "Each section will delve into a specific activation function, explaining its mathematical definition, rationale for use, common applications (including the types of problems they are well-suited for), and visualizing its behavior."
      ],
      "id": "ad20c68e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYPL6SAOF1_0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "id": "EYPL6SAOF1_0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b94b7f50"
      },
      "source": [
        "## ReLU (Rectified Linear Unit)\n",
        "**Explanation:**\n",
        "The Rectified Linear Unit (ReLU) is a piecewise linear function that outputs the input directly if it is positive, and outputs zero otherwise. Mathematically, it is defined as $f(x) = \\max(0, x)$.\n",
        "**Rationale for use:**\n",
        "- **Computational Efficiency:** ReLU is computationally inexpensive as it only involves a simple thresholding operation. This speeds up the training process compared to sigmoid or tanh functions.\n",
        "- **Accelerated Convergence:** ReLU helps deep neural networks converge faster during training.\n",
        "- **Mitigation of Vanishing Gradients:** For positive inputs, the gradient of ReLU is a constant 1, which helps to combat the vanishing gradient problem that can occur with sigmoid or tanh in deep networks.\n",
        "**Common applications:** Image classification (e.g., in Convolutional Neural Networks), Natural Language Processing.\n",
        "**Example:**\n",
        "If the input to a ReLU neuron is 5, the output is $\\max(0, 5) = 5$.\n",
        "If the input is -3, the output is $\\max(0, -3) = 0$."
      ],
      "id": "b94b7f50"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTvZwJlvF1_2"
      },
      "outputs": [],
      "source": [
        "x = np.linspace(-10, 10, 400)\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "plt.plot(x, relu(x), label='ReLU', linewidth=2)\n",
        "plt.title('ReLU Activation Function')\n",
        "plt.axhline(0, color='gray', linestyle='--')\n",
        "plt.axvline(0, color='gray', linestyle='--')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "mTvZwJlvF1_2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70affb4d"
      },
      "source": [
        "## Sigmoid Activation Function\n",
        "**Explanation:**\n",
        "The sigmoid function, also known as the logistic function, is a non-linear function that maps any input value to a value between 0 and 1. It is defined as $f(x) = \\frac{1}{1 + e^{-x}}$.\n",
        "**Rationale for use:**\n",
        "- **Output Layer for Binary Classification:** The sigmoid function is commonly used in the output layer of neural networks for binary classification problems because its output can be interpreted as a probability.\n",
        "- **Smooth Gradient:** The sigmoid function has a smooth gradient, which can be beneficial during backpropagation.\n",
        "**Common applications:** Output layers in binary classification models, logistic regression.\n",
        "**Example:**\n",
        "If the input is 2, the output is $\\frac{1}{1 + e^{-2}} \\approx 0.88$.\n",
        "If the input is -1, the output is $\\frac{1}{1 + e^{1}} \\approx 0.27$."
      ],
      "id": "70affb4d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZ1PCp52F1_2"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "plt.plot(x, sigmoid(x), label='Sigmoid', linewidth=2)\n",
        "plt.title('Sigmoid Activation Function')\n",
        "plt.axhline(0, color='gray', linestyle='--')\n",
        "plt.axvline(0, color='gray', linestyle='--')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "TZ1PCp52F1_2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37e778d0"
      },
      "source": [
        "## Tanh Activation Function\n",
        "**Explanation:**\n",
        "The hyperbolic tangent (tanh) function is a non-linear function that maps any input value to a value between -1 and 1. It is defined as $f(x) = \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$.\n",
        "**Rationale for use:**\n",
        "- **Zero-Centered Output:** The output of the tanh function is zero-centered, which can make training more stable and can lead to faster convergence compared to the sigmoid function.\n",
        "- **Smoother Gradient:** Like sigmoid, tanh has a smooth gradient.\n",
        "**Common applications:** Recurrent Neural Networks (RNNs), hidden layers in neural networks.\n",
        "**Example:**\n",
        "If the input is 2, the output is $\\tanh(2) \\approx 0.96$.\n",
        "If the input is -1, the output is $\\tanh(-1) \\approx -0.76$."
      ],
      "id": "37e778d0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziibtVaIF1_2"
      },
      "outputs": [],
      "source": [
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "plt.plot(x, tanh(x), label='Tanh', linewidth=2)\n",
        "plt.title('Tanh Activation Function')\n",
        "plt.axhline(0, color='gray', linestyle='--')\n",
        "plt.axvline(0, color='gray', linestyle='--')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "ziibtVaIF1_2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05638d88"
      },
      "source": [
        "## Leaky ReLU Activation Function\n",
        "**Explanation:**\n",
        "Leaky ReLU is a variation of the ReLU function that allows a small, non-zero gradient when the input is negative. It is defined as $f(x) = \\max(\\alpha x, x)$, where $\\alpha$ is a small positive constant (typically 0.01).\n",
        "**Rationale for use:**\n",
        "- **Addresses the \"Dying ReLU\" Problem:** By allowing a small gradient for negative inputs, Leaky ReLU prevents neurons from becoming inactive (dying) when their input is negative, which can happen with standard ReLU.\n",
        "**Common applications:** Deeper models that may suffer from the \"dying ReLU\" problem.\n",
        "**Example:**\n",
        "If the input is 5 and $\\alpha = 0.01$, the output is $\\max(0.01 \\times 5, 5) = 5$.\n",
        "If the input is -3 and $\\alpha = 0.01$, the output is $\\max(0.01 \\times -3, -3) = \\max(-0.03, -3) = -0.03$."
      ],
      "id": "05638d88"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqBWJVNhF1_3"
      },
      "outputs": [],
      "source": [
        "def leaky_relu(x, alpha=0.01):\n",
        "    return np.where(x > 0, x, alpha * x)\n",
        "plt.plot(x, leaky_relu(x), label='Leaky ReLU', linewidth=2)\n",
        "plt.title('Leaky ReLU Activation Function')\n",
        "plt.axhline(0, color='gray', linestyle='--')\n",
        "plt.axvline(0, color='gray', linestyle='--')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "oqBWJVNhF1_3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5800db2"
      },
      "source": [
        "## ELU Activation Function\n",
        "**Explanation:**\n",
        "The Exponential Linear Unit (ELU) is an activation function that outputs the input for non-negative values and a scaled exponential function for negative values. It is defined as $f(x) = \\begin{cases} x & \\text{if } x \\ge 0 \\\\ \\alpha (e^x - 1) & \\text{if } x < 0 \\end{cases}$, where $\\alpha$ is a positive constant (typically 1.0).\n",
        "**Rationale for use:**\n",
        "- **Smooth Transition:** ELU provides a smooth transition between negative and positive values.\n",
        "- **Helps Maintain Small Gradients:** The negative part of the ELU function helps to push the mean activation towards zero, which can help to maintain small gradients and lead to faster learning.\n",
        "**Common applications:** Deep neural networks.\n",
        "**Example:**\n",
        "If the input is 5 and $\\alpha = 1.0$, the output is 5.\n",
        "If the input is -3 and $\\alpha = 1.0$, the output is $1.0 \\times (e^{-3} - 1) \\approx 1.0 \\times (0.05 - 1) = -0.95$."
      ],
      "id": "e5800db2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtMP1vqIF1_3"
      },
      "outputs": [],
      "source": [
        "def elu(x, alpha=1.0):\n",
        "    return np.where(x >= 0, x, alpha * (np.exp(x) - 1))\n",
        "plt.plot(x, elu(x), label='ELU', linewidth=2)\n",
        "plt.title('ELU Activation Function')\n",
        "plt.axhline(0, color='gray', linestyle='--')\n",
        "plt.axvline(0, color='gray', linestyle='--')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "gtMP1vqIF1_3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "debb5300"
      },
      "source": [
        "## Swish Activation Function\n",
        "**Explanation:**\n",
        "Swish is a smooth, non-monotonic activation function defined as $f(x) = x \\cdot \\text{sigmoid}(x)$. It is also known as the Self-Gated Activation Function.\n",
        "**Rationale for use:**\n",
        "- **Smooth and Flexible:** The smooth nature of Swish leads to smoother gradients, which can improve optimization.\n",
        "- **Better Performance:** Swish has been shown to outperform ReLU on deeper models in some cases.\n",
        "**Common applications:** EfficientNet and other deep learning models.\n",
        "**Example:**\n",
        "If the input is 2, the output is $2 \\times \\text{sigmoid}(2) \\approx 2 \\times 0.88 = 1.76$.\n",
        "If the input is -1, the output is $-1 \\times \\text{sigmoid}(-1) \\approx -1 \\times 0.27 = -0.27$."
      ],
      "id": "debb5300"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-QNKX-cF1_4"
      },
      "outputs": [],
      "source": [
        "def swish(x):\n",
        "    return x * sigmoid(x)\n",
        "plt.plot(x, swish(x), label='Swish', linewidth=2)\n",
        "plt.title('Swish Activation Function')\n",
        "plt.axhline(0, color='gray', linestyle='--')\n",
        "plt.axvline(0, color='gray', linestyle='--')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "J-QNKX-cF1_4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4d80594"
      },
      "source": [
        "## Softplus Activation Function\n",
        "**Explanation:**\n",
        "The Softplus function is a smooth approximation of the ReLU function. It is defined as $f(x) = \\ln(1 + e^x)$.\n",
        "**Rationale for use:**\n",
        "- **Smooth and Differentiable:** Softplus is differentiable everywhere, unlike ReLU which has a sharp corner at zero. This can be beneficial in certain optimization algorithms.\n",
        "- **Always Positive:** The output of the Softplus function is always positive.\n",
        "**Common applications:** Probabilistic models, networks where differentiability is required for stability.\n",
        "**Example:**\n",
        "If the input is 2, the output is $\\ln(1 + e^2) \\approx \\ln(1 + 7.39) = \\ln(8.39) \\approx 2.13$.\n",
        "If the input is -1, the output is $\\ln(1 + e^{-1}) \\approx \\ln(1 + 0.37) = \\ln(1.37) \\approx 0.31$."
      ],
      "id": "e4d80594"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbGMVe0aF1_4"
      },
      "outputs": [],
      "source": [
        "def softplus(x):\n",
        "    return np.log(1 + np.exp(x))\n",
        "plt.plot(x, softplus(x), label='Softplus', linewidth=2)\n",
        "plt.title('Softplus Activation Function')\n",
        "plt.axhline(0, color='gray', linestyle='--')\n",
        "plt.axvline(0, color='gray', linestyle='--')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "hbGMVe0aF1_4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89c44d5b"
      },
      "source": [
        "## Understanding Neuron Outputs and the Vanishing Gradient Problem\n",
        "\n",
        "In a neural network, the output of a neuron after applying an activation function can vary depending on the input and the specific function used.\n",
        "\n",
        "*   **Output of 0 or close to 0:** An output of 0 or a value close to it often signifies that the neuron is \"inactive\" or has a very weak signal for a given input. For activation functions like standard ReLU, an output of 0 means the input was negative, and the neuron is not contributing to the forward pass beyond that point. For sigmoid or tanh, values close to 0 are in the flatter regions of the curve.\n",
        "\n",
        "*   **Output of -1 or other negative numbers:** Activation functions like Tanh and Leaky ReLU can produce negative outputs. A negative output indicates a negative contribution to the subsequent layer. In Tanh, an output of -1 represents a strongly negative input. In Leaky ReLU, a small negative output for a negative input helps maintain a small gradient.\n",
        "\n",
        "*   **Output of positive numbers:** Positive outputs generally indicate that the neuron is \"active\" and contributing a positive signal. For ReLU and ELU, positive inputs result in positive outputs. For sigmoid and tanh, positive outputs are produced for positive inputs, with values approaching 1 (sigmoid) or -1 (tanh) as the input becomes very large or very small, respectively.\n",
        "\n",
        "### The Vanishing Gradient Problem\n",
        "\n",
        "The **vanishing gradient problem** is a phenomenon that occurs during the training of deep neural networks using gradient-based optimization (like backpropagation). It happens when the gradients of the loss function with respect to the weights become extremely small as they are propagated backward through the network's layers.\n",
        "\n",
        "**Why it's problematic:**\n",
        "\n",
        "*   **Slow Learning in Early Layers:** Small gradients mean that the weight updates in the earlier layers of the network are tiny. This significantly slows down the learning process for these layers, which are crucial for learning low-level features from the input data.\n",
        "*   **Difficulty Capturing Long-Range Dependencies:** In recurrent neural networks (RNNs), vanishing gradients make it difficult for the network to learn dependencies between data points that are far apart in a sequence.\n",
        "\n",
        "**How some activation functions help:**\n",
        "\n",
        "Activation functions with gradients that do not become extremely small for large inputs are better at mitigating the vanishing gradient problem.\n",
        "\n",
        "*   **ReLU:** For positive inputs ($x > 0$), the gradient of ReLU is a constant 1. This prevents the gradient from shrinking as it passes through layers with positive activations, effectively combating vanishing gradients in these cases. However, for negative inputs ($x \\le 0$), the gradient is 0, leading to the \"dying ReLU\" problem where neurons can become permanently inactive.\n",
        "*   **Leaky ReLU:** By introducing a small slope ($\\alpha$) for negative inputs, Leaky ReLU ensures that the gradient is never exactly zero for any input value. This prevents the \"dying ReLU\" problem and helps maintain a small gradient even for negative activations, further mitigating vanishing gradients compared to standard ReLU.\n",
        "*   **ELU:** ELU has a non-zero gradient for negative inputs, providing a smooth transition and helping to push the mean activation towards zero, which can also contribute to faster learning and alleviate vanishing gradients.\n",
        "*   **Swish:** Swish has a smooth, non-monotonic shape and a non-zero gradient across its domain, which can also help with gradient flow compared to ReLU.\n",
        "\n",
        "In contrast, activation functions like **Sigmoid** and **Tanh** suffer more from vanishing gradients because their gradients are very close to zero for large positive or negative inputs (in the saturated regions of their curves). As gradients are multiplied during backpropagation, passing through layers with saturated sigmoid or tanh activations can cause the overall gradient to shrink exponentially. Stay tuned, as I'll be introducing more information about backpropogation, gradient descent, loss and optimizers.  "
      ],
      "id": "89c44d5b"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}